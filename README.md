# Minimal Transformer

ä»é›¶å¼€å§‹å®ç° Transformerï¼Œé€æ­¥æ„å»º **"Attention Is All You Need"** è®ºæ–‡ä¸­çš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ã€‚

æœ¬é¡¹ç›®ä¸“ä¸ºæ•™å­¦è®¾è®¡ï¼Œæ¯ä¸ªæ¨¡å—éƒ½åŒ…å«è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šå’Œå¯è¿è¡Œçš„ç¤ºä¾‹ã€‚

## ğŸ“– é¡¹ç›®ç»“æ„

```
minimal-transformer/
â”‚
â”œâ”€â”€ attention/                          # ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ scaled_dot_product_attention.py # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆåŸºç¡€ï¼‰
â”‚   â”œâ”€â”€ self_attention.py               # è‡ªæ³¨æ„åŠ›
â”‚   â”œâ”€â”€ cross_attention.py              # äº¤å‰æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ masked_self_attention.py        # å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›
â”‚   â”œâ”€â”€ multi_head_attention_core.py    # å¤šå¤´æ³¨æ„åŠ›æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ multi_head_self_attention.py    # å¤šå¤´è‡ªæ³¨æ„åŠ›
â”‚   â”œâ”€â”€ multi_head_cross_attention.py   # å¤šå¤´äº¤å‰æ³¨æ„åŠ›
â”‚   â””â”€â”€ multi_head_masked_self_attention.py # å¤šå¤´å¸¦æ©ç è‡ªæ³¨æ„åŠ›
â”‚
â”œâ”€â”€ layers/                             # ğŸ§± ç½‘ç»œå±‚ç»„ä»¶
â”‚   â”œâ”€â”€ feed_forward.py                 # ä½ç½®å‰é¦ˆç½‘ç»œ (FFN)
â”‚   â”œâ”€â”€ residual_layer_norm.py          # æ®‹å·®è¿æ¥ & å±‚å½’ä¸€åŒ–
â”‚   â””â”€â”€ positional_encoding.py          # ä½ç½®ç¼–ç 
â”‚
â”œâ”€â”€ models/                             # ğŸ—ï¸ å®Œæ•´æ¨¡å‹
â”‚   â”œâ”€â”€ encoder.py                      # Transformer Encoder
â”‚   â”œâ”€â”€ decoder.py                      # Transformer Decoder
â”‚   â””â”€â”€ transformer.py                  # å®Œæ•´ Transformer
â”‚
â””â”€â”€ utils.py                            # ğŸ”§ å·¥å…·å‡½æ•°
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
pip install torch matplotlib numpy
```

### è¿è¡Œç¤ºä¾‹

æ¯ä¸ªæ¨¡å—éƒ½å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼ŒåŒ…å«è¯¦ç»†çš„æ¼”ç¤ºï¼š

```bash
# 1. ç†è§£ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
python attention/scaled_dot_product_attention.py

# 2. ç†è§£ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶
python attention/self_attention.py
python attention/cross_attention.py
python attention/masked_self_attention.py

# 3. ç†è§£å¤šå¤´æ³¨æ„åŠ›
python attention/multi_head_self_attention.py

# 4. ç†è§£å…¶ä»–ç»„ä»¶
python layers/feed_forward.py
python layers/positional_encoding.py
python layers/residual_layer_norm.py

# 5. è¿è¡Œå®Œæ•´ Transformer
python models/transformer.py
```

## ğŸ“ å­¦ä¹ è·¯å¾„

å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå­¦ä¹ ï¼š

### ç¬¬ä¸€é˜¶æ®µï¼šæ³¨æ„åŠ›æœºåˆ¶åŸºç¡€

```
scaled_dot_product_attention.py
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
self_attention  cross_attention
    â†“
masked_self_attention
```

**æ ¸å¿ƒå…¬å¼**ï¼š
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### ç¬¬äºŒé˜¶æ®µï¼šå¤šå¤´æ³¨æ„åŠ›

```
multi_head_attention_core.py  â† å…¬å…±å®ç°
         â†“
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
    â†“    â†“    â†“
  è‡ªæ³¨æ„åŠ›  äº¤å‰æ³¨æ„åŠ›  å¸¦æ©ç è‡ªæ³¨æ„åŠ›
```

**æ ¸å¿ƒæ€æƒ³**ï¼šå¤šä¸ªæ³¨æ„åŠ›å¤´ä»ä¸åŒè§’åº¦ç†è§£åºåˆ—å…³ç³»

### ç¬¬ä¸‰é˜¶æ®µï¼šè¾…åŠ©ç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| ä½ç½®ç¼–ç  | æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼ˆæ³¨æ„åŠ›æœ¬èº«æ˜¯ç½®æ¢ä¸å˜çš„ï¼‰ |
| å‰é¦ˆç½‘ç»œ | éçº¿æ€§å˜æ¢ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ› |
| æ®‹å·®è¿æ¥ | ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œå¸®åŠ©è®­ç»ƒæ·±å±‚ç½‘ç»œ |
| å±‚å½’ä¸€åŒ– | ç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•› |

### ç¬¬å››é˜¶æ®µï¼šå®Œæ•´æ¨¡å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Transformer                           â”‚
â”‚                                                             â”‚
â”‚  æºåºåˆ— â”€â”€â†’ [Embedding + PE] â”€â”€â†’ Encoder â”€â”€â”               â”‚
â”‚                                            â†“               â”‚
â”‚  ç›®æ ‡åºåˆ— â”€â”€â†’ [Embedding + PE] â”€â”€â†’ Decoder â”€â”€â†’ Output      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶

| ç±»å‹ | Q æ¥æº | K, V æ¥æº | æ©ç  | ç”¨é€” |
|------|--------|-----------|------|------|
| è‡ªæ³¨æ„åŠ› | è¾“å…¥åºåˆ— | è¾“å…¥åºåˆ— | å¯é€‰ | Encoder |
| äº¤å‰æ³¨æ„åŠ› | Decoder | Encoder | å¯é€‰ | Decoder å…³æ³¨ Encoder |
| å¸¦æ©ç è‡ªæ³¨æ„åŠ› | è¾“å…¥åºåˆ— | è¾“å…¥åºåˆ— | å› æœæ©ç  | Decoder è‡ªå›å½’ç”Ÿæˆ |

### Transformer ç»“æ„ (åŸè®ºæ–‡)

```
Encoder Layer:                    Decoder Layer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Self-Attention   â”‚              â”‚ Masked           â”‚
â”‚ + Add & Norm     â”‚              â”‚ Self-Attention   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚ + Add & Norm     â”‚
â”‚ Feed Forward     â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + Add & Norm     â”‚              â”‚ Cross-Attention  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ + Add & Norm     â”‚
                                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                  â”‚ Feed Forward     â”‚
                                  â”‚ + Add & Norm     â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ ä»£ç ç¤ºä¾‹

### ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›

```python
from attention import MultiHeadSelfAttention

# åˆ›å»ºæ¨¡å—
mhsa = MultiHeadSelfAttention(embed_size=512, num_heads=8)

# è¾“å…¥: (batch_size, seq_len, embed_size)
x = torch.randn(2, 10, 512)

# å‰å‘ä¼ æ’­
output, attention_weights = mhsa(x)
# output: (2, 10, 512)
# attention_weights: (2, 8, 10, 10)
```

### ä½¿ç”¨å®Œæ•´ Transformer

```python
from models import Transformer

# åˆ›å»ºæ¨¡å‹
model = Transformer(
    src_vocab_size=10000,
    tgt_vocab_size=10000,
    d_model=512,
    num_heads=8,
    num_layers=6,
    d_ff=2048
)

# è¾“å…¥
src = torch.randint(0, 10000, (2, 20))  # æºåºåˆ—
tgt = torch.randint(0, 10000, (2, 15))  # ç›®æ ‡åºåˆ—

# å‰å‘ä¼ æ’­
logits = model(src, tgt)
# logits: (2, 15, 10000)
```

## ğŸ“– å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸå§‹è®ºæ–‡
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - å¯è§†åŒ–è®²è§£
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - ä»£ç æ³¨è§£

## ğŸ“„ License

MIT License
